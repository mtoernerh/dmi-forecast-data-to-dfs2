{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03645f67-fe1e-454f-9650-478965d52c79",
   "metadata": {},
   "source": [
    "In this notebook, we will cover downloading dmi harmonie forecasts and processing to a dfs2 file format used by MIKE SHE for hydrogeological modelling.\n",
    "We will use the standard python libaries os, sys, time, glob, traceback, copy and datetime, along with third party libaries numpy, pandas, xarray, matplotlib, requests, pytz, pyproj, rasterio, and mikeio.\n",
    "\n",
    "## Step 1: Install Conda\n",
    "\n",
    "If you haven't already, install Anaconda or Miniconda. You can download them from [Anaconda](https://anaconda.org/) or [Miniconda](https://docs.anaconda.com/miniconda/).\n",
    "\n",
    "## Step 2: Create a New Conda Environment\n",
    "\n",
    "Open Anaconda prompt, then run the following command to create a new environment. We’ll name it hydro_forecast_env, but feel free to use any name you like:  \n",
    "`conda create -n hydro_forecast_env python=3.11`\n",
    "\n",
    "This command will create an environment with Python 3.11, which is compatible with all the libraries we’ll use.\n",
    "\n",
    "## Step 3: Activate the Environment\n",
    "`conda activate hydro_forecast_env`\n",
    "\n",
    "## Step 4: Install Required Packages\n",
    "Now that the environment is active, we’ll install the required libraries. Some libraries can be installed directly via Conda, while others require pip.  \n",
    "`conda install numpy pandas xarray matplotlib requests pytz pyproj rasterio -c conda-forge`\n",
    "\n",
    "mikeio is specific to MIKE SHE and not available through Conda, so we’ll use pip to install it:\n",
    "`pip install mikeio`\n",
    "\n",
    "## Step 5: Verify the Installation\n",
    "To make sure everything is correctly installed, open a Python interpreter in the Conda environment and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92db6b0-0936-44a0-bba9-3066c48487ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os, sys, time, glob, traceback, copy\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "# Third-Party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pytz\n",
    "from pyproj import CRS\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "# MIKE IO imports\n",
    "import mikeio\n",
    "from mikeio import ItemInfo, EUMType, EUMUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef8846-5974-45b1-a961-91030d773636",
   "metadata": {},
   "source": [
    "Before we can access harmonie forecast data we must create an account following this [guide](https://opendatadocs.dmi.govcloud.dk/en/Authentication) and request an API key for forecastedr. Then we define a string variable using our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7721f6c2-9a93-47d4-a709-30ed7ab6a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '<insert api key here>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc998907-e891-4430-be2a-74e5136a6d88",
   "metadata": {},
   "source": [
    "Next, we will dynamically identify the closest available forecast based on the current time and create a model run ID. This assumes each forecast becomes available approximately six hours after its scheduled time. Alternatively, when making the API call later, you can omit the model run ID to automatically download the latest available forecast. Using the same syntax, we could also iterate over filtered forecasts to download each forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b048dfa-75d3-4c50-99a0-3f394c06a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up forecast hours and date for today and yesterday\n",
    "date_today = datetime.today().date()\n",
    "date_yesterday = date_today - timedelta(days=1)\n",
    "forecast_hours = ['00', '06', '12', '18']\n",
    "\n",
    "\n",
    "# Determine the cutoff time (six hours prior to now)\n",
    "tback = datetime.now() - timedelta(hours=6)\n",
    "\n",
    "# Generate forecast times for today and yesterday\n",
    "forecast_datetimes = [\n",
    "    datetime.combine(date, datetime.strptime(hour, '%H').time())\n",
    "    for date in [date_yesterday, date_today]\n",
    "    for hour in forecast_hours\n",
    "]\n",
    "\n",
    "# Filter forecast times that are fully uploaded\n",
    "filtered_forecasts = [h for h in forecast_datetimes if h < tback]\n",
    "\n",
    "# Find the closest forecast time to tback\n",
    "closest_forecast = max(filtered_forecasts, default=tback)\n",
    "\n",
    "# Create modelrun ID\n",
    "modelrun = f'{date_today}T{closest_forecast.strftime(\"%H\")}0000Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962f8af7-5641-4886-b591-36b9d78f1755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-08T000000Z'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3046277-6824-4ecb-b30f-46fffd1d6e82",
   "metadata": {},
   "source": [
    "Next, we’ll write the API call to download the model results. We’ll use a dictionary to organize the various components of the request. First, we specify the **API key**, followed by the **endpoint**, which directs to the EDR API for DMI. The **collection** points to the DINI HARMONIE model, and the **instance** references the specific model run using our dynamically generated model ID. The **bbox** targets a bounding box around Denmark, and **crs** specifies the coordinate reference system. We include relevant **parameters** needed for calculating potential evapotranspiration and precipitation. Finally, we set the **format** to NetCDF for easy data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18de64a8-2605-46b4-8b75-d4251511a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_dict = {\n",
    "\"apikey\": f\"api-key={api_key}\",\n",
    "\"endpoint\": r\"https://dmigw.govcloud.dk/v1/forecastedr\",\n",
    "\"collection\": r\"/collections/harmonie_dini_sf/\",\n",
    "\"instance\": fr\"instances/{modelrun}/\", \n",
    "\"bbox\": \"bbox?bbox=5.03,53.21,17.5,58.57\",\n",
    "\"crs\": \"crs=crs84\",\n",
    "\"parameter\": [\n",
    "    \"temperature-2m\",\n",
    "    \"wind-speed\",\n",
    "    \"total-precipitation\",\n",
    "    \"pressure-surface\",\n",
    "    \"relative-humidity-2m\",\n",
    "    \"net-short-wave-radiation-flux\",\n",
    "    \"net-long-wave-radiation-flux\",\n",
    "    \"global-radiation-flux\"\n",
    "],\n",
    "\"format\": \"f=NetCDF\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc02f70-763b-4803-a1c9-30284a613ef5",
   "metadata": {},
   "source": [
    "In this step, we’ll create a directory to store the downloaded NetCDF files. Using **os.makedirs**, we specify an **output folder** in the current working directory, ensuring it exists or is created if missing.\n",
    "\n",
    "We then proceed to download the forecast data for each specified parameter. For each parameter in our **api_dict**, we construct the API request URL by combining the endpoint, collection, instance, bounding box, parameter name, coordinate reference system, file format, and API key.\n",
    "\n",
    "For each parameter, we send a request to download its data in NetCDF format. If successful, we save the file to our output folder with a filename that includes the parameter name (e.g., 'harmonie_temperature-2m.nc'). If any request fails, an error message is printed, allowing us to troubleshoot connectivity issues with the EDR API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7603a753-2178-46a1-b14e-b36477deb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving harmonie_temperature-2m.nc\n",
      "Saving harmonie_wind-speed.nc\n",
      "Saving harmonie_total-precipitation.nc\n",
      "Saving harmonie_pressure-surface.nc\n",
      "Saving harmonie_relative-humidity-2m.nc\n",
      "Saving harmonie_net-short-wave-radiation-flux.nc\n",
      "Saving harmonie_net-long-wave-radiation-flux.nc\n",
      "Saving harmonie_global-radiation-flux.nc\n"
     ]
    }
   ],
   "source": [
    "# Create a folder for storing netcdfs\n",
    "output_folder = fr'{os.getcwd()}\\output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Download parameter forecast to netcdfs\n",
    "try:\n",
    "    for par in api_dict[\"parameter\"]:\n",
    "        entry_url = f\"{api_dict['endpoint']}{api_dict['collection']}{api_dict['instance']}{api_dict['bbox']}&parameter-name={par}&{api_dict['crs']}&{api_dict['format']}&{api_dict['apikey']}\"\n",
    "        response  = requests.get(entry_url)\n",
    "        response.raise_for_status()\n",
    "                \n",
    "        netcdf_file = os.path.join(output_folder, f\"harmonie_{par}.nc\")\n",
    "                \n",
    "        with open(netcdf_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print( f\"Saving harmonie_{par}.nc\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to connect to EDR API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec94a2-1167-4640-bc72-f099430bacfc",
   "metadata": {},
   "source": [
    "## 1. Parameter Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61decab0-4332-4654-9879-2619b84ed713",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'temp': [EUMType.Temperature, EUMUnit.degree_Celsius, 'Temperature', 'Instantaneous', 'temperature-2m'],\n",
    "    'pres': [EUMType.Pressure, EUMUnit.kilopascal, 'Pressure', 'Instantaneous', 'pressure-surface'],\n",
    "    'grad': [EUMType.Sun_radiation, EUMUnit.kJ_per_meter_pow_2_per_hour, 'Global radiation', 'MeanStepBackward', 'global-radiation-flux'], \n",
    "    'precip': [EUMType.Precipitation_Rate, EUMUnit.mm_per_hour, 'Precipitation rate', 'MeanStepBackward', 'total-precipitation'], \n",
    "    'ws': [EUMType.Wind_speed, EUMUnit.meter_per_sec, 'Wind speed', 'Instantaneous', 'wind-speed'],\n",
    "    'rh': [EUMType.Relative_humidity, EUMUnit.percent, 'Relative humidity', 'Instantaneous', 'relative-humidity-2m'],\n",
    "    'nlwrf': [EUMType.Sun_radiation, EUMUnit.kJ_per_meter_pow_2_per_hour, 'Net longwave radiation flux', 'MeanStepBackward', 'net-long-wave-radiation-flux'], \n",
    "    'nswrf': [EUMType.Sun_radiation, EUMUnit.kJ_per_meter_pow_2_per_hour, 'Net shortwave radiation flux', 'MeanStepBackward', 'net-short-wave-radiation-flux'],\n",
    "    'makkink': [EUMType.Evapotranspiration_Rate, EUMUnit.mm_per_hour, 'Potential evapotranspiration', 'MeanStepBackward'],\n",
    "    'penman_monteith': [EUMType.Evapotranspiration_Rate, EUMUnit.mm_per_hour, 'Potential evapotranspiration', 'MeanStepBackward'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b6068-b857-48a4-ae4a-6f9b9d9abcc3",
   "metadata": {},
   "source": [
    "This dictionary maps each parameter (e.g., temperature, pressure, etc.) to a list containing information about:\n",
    "- The mikeio type (e.g., Temperature, Pressure).\n",
    "- The mikeio unit (e.g., degree Celsius, kilopascal).\n",
    "- A mikeio item name (e.g., 'Temperature', 'Pressure').\n",
    "- A mikeio step type (e.g., 'Instantaneous', 'MeanStepBackward').\n",
    "- The initial parameter name used in the NetCDF files (e.g., 'temperature-2m').\n",
    "\n",
    "These are not the initial units, but the units we will save the dfs2 files with. To see the initial units [check here.](https://opendatadocs.dmi.govcloud.dk/Data/Forecast_Data_Weather_Model_HARMONIE_DINI_IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea68b9-b092-42ab-bcf6-ba2ae1762cda",
   "metadata": {},
   "source": [
    "## 2. Opening NetCDF Files as xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b32d52-bcce-4c9e-bc63-e06c5533636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "netcdf_files = glob(f'{output_folder}/*.nc')\n",
    "ds = xr.open_mfdataset(netcdf_files, combine='by_coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2cba6-346c-4270-87b2-b0620fdab8e8",
   "metadata": {},
   "source": [
    "This uses the glob function to get a list of all .nc (NetCDF) files in the output_folder.  \n",
    "The open_mfdataset function from xarray is used to load all the NetCDF files into a single dataset, combining them by coordinates (assuming they share the same grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0312768-d8ca-4b80-91af-a46669b5ad6a",
   "metadata": {},
   "source": [
    "## 3. Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e9795-938d-4803-8bac-dbd4a809dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming\n",
    "rename_dict = {values[-1]: new_name for new_name, values in param.items() if len(values) > 4} \n",
    "ds = ds.rename(rename_dict)\n",
    "\n",
    "# Assigning custom CRS\n",
    "dmi_harmonie_crs = CRS.from_proj4(\"+proj=lcc +lat_1=55.5 +lat_2=55.5 +lat_0=55.5 \"\n",
    "                                  \"+lon_0=-8 +x_0=0 +y_0=0 +a=6371229 +b=6371229 +units=m +no_defs\")\n",
    "ds.rio.write_crs(\n",
    "    dmi_harmonie_crs.to_wkt(), \n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Assigning spatial dimensions\n",
    "ds.rio.set_spatial_dims(\n",
    "    x_dim='projection_x_coordinate', \n",
    "    y_dim='projection_y_coordinate', \n",
    "    inplace=True\n",
    ")\n",
    "# Reproject dataset to UTM32N\n",
    "ds = ds.rio.reproject(\n",
    "    25832, \n",
    "    resolution=2000, \n",
    "    resampling=Resampling.bilinear\n",
    ")\n",
    "# Sptial subsetting\n",
    "mask_x = np.logical_and(ds.x >= 440000, ds.x <= 900000)\n",
    "mask_y = np.logical_and(ds.y >= 6035000, ds.y <= 6484487)\n",
    "mask = np.logical_and(mask_x, mask_y)\n",
    "ds = ds.where(mask, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396a092-f31a-4110-b982-799e349aa1b6",
   "metadata": {},
   "source": [
    "A `rename_dict` is created that maps the parameter name in the NetCDF files (e.g., 'temperature-2m') to a more readable name (e.g., 'temp') from the param dictionary. This rename operation is then applied to the dataset to ensure that the variables in the dataset have more user-friendly names. \n",
    "\n",
    "A custom Lambert Conformal Conic (LCC) projection using the PROJ.4 format is created and assigned to the dataset using the `rio.write_crs` function from rioxarray. The `to_wkt()` method converts the CRS to Well-Known Text (WKT) format, which is the required input for this function.  \n",
    "\n",
    "The `set_spatial_dims` function is used to specify which dimensions represent the spatial coordinates (i.e., the X and Y axes). Here, it’s assumed that the dataset uses 'projection_x_coordinate' and 'projection_y_coordinate' for the horizontal axes.  \n",
    "The dataset is reprojected to UTM Zone 32N (EPSG:25832) using the `rio.reproject` function. The resolution is set to 2000 meters, and the resampling method used for interpolation is bilinear, which is appropriate for continuous data like temperature or precipitation.  \n",
    "\n",
    "Masking is done to limit the dataset to a specific geographical region. The bounds for the X and Y coordinates are set to cover the area of interest, which in this case Denmark. The `where` function is used to apply the mask, dropping data outside the specified region.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91f50c-d328-40d7-ab8a-a854d4cbcedf",
   "metadata": {},
   "source": [
    "## 4. Unit conversion and de-accumulating the accumulated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872eb6df-5273-4552-adbb-1a76464f17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert units\n",
    "ds['pres'] = ds['pres'] * 1E-3   # Pa to kPa\n",
    "ds['temp'] = ds['temp'] - 273.15 # Kelvin to Celsius \n",
    "        \n",
    "# Create a list of accumulated parameters to de-accumulate\n",
    "accumulated_vars = ['grad', 'nswrf', 'nlwrf', 'precip']\n",
    "    \n",
    "# De-accumulating the accumulated parameters\n",
    "for var in accumulated_vars:\n",
    "    data = ds[var].data                     # Extract original data\n",
    "    ds[var].data[1:] = data[1:] - data[:-1] # Calculate difference with previous timestep\n",
    "        \n",
    "# Calculate net radiation and convert units for PET calculations\n",
    "ds['rn']   = ds['nswrf'] + ds['nlwrf'] # Calculate net radiation\n",
    "ds['grad'] = ds['grad'] * 1E-6         # Joul to Megajoul            \n",
    "ds['rn']   = ds['rn']   * 1E-6         # Calculate Net radiation and to Megajoul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e2cd3-3df2-4ab4-9ac5-605332ca6399",
   "metadata": {},
   "source": [
    "In preperation to calculating Penman-monteith and Makkink potential evapotranspiration, units are converted to match the FAO [guideline](https://www.fao.org/4/x0490e/x0490e05.htm#TopOfPage). The pressure (`pres`) variable is converted from Pascals (Pa) to kilopascals (kPa) by multiplying by 1 x 10-3 and temperature (`temp`) is coverted from Kelvin to Celsius by subtracting 273.15.   \n",
    "For variables representing accumulated values (`grad`, `precip`, `nswrf`, `nlwrf`), the values are \"de-accumulated\" to show instantaneous changes per timestep, rather than cumulative totals. This is done by subtracting each timestep's value from the previous timestep. Spurious positive or negative values can sometimes appear in the de-accumulated values due to rounding in accumulation processes; this behavior is discussed in the ECMW [documentation](https://confluence.ecmwf.int/display/UDOC/Why+are+there+sometimes+small+negative+precipitation+accumulations+-+ecCodes+GRIB+FAQ).  \n",
    "  \n",
    "For the Penman-Monteith calculation, net radiation (`rn`) is computed as the sum of net shortwave (`nswrf`) and net longwave radiation fluxes (`nlwrf`). Finally, both `grad` and `rn` are converted from joules to megajoules by multiplying by 1 x 10−6 to match conventional units for evapotranspiration calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb43ff-755b-4a68-9423-6e0e49cb039a",
   "metadata": {},
   "source": [
    "## 5. Calculating Penman-monteith and Makkink PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410587e-c381-4d2b-b984-cfe88d94b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daylight_factors(timeseries, timezone = 'Europe/Copenhagen', lat = 55.686, lng = 12.570):\n",
    "    \"\"\" \n",
    "    Calculates daylight factors (sunrise, sunset) for each timestep in the timeseries.\n",
    "    \"\"\"\n",
    "    daylight_factors = []\n",
    "    copenhagen_tz = pytz.timezone(timezone)\n",
    "    for timestep in timeseries:\n",
    "        ts_datetime = pd.to_datetime(timestep)\n",
    "        \n",
    "        # Get sunrise and sunset times\n",
    "        response = requests.get('https://api.sunrise-sunset.org/json', \n",
    "                                params={'lat':lat, 'lng': lng, 'date': ts_datetime.date()})\n",
    "        r = response.json()['results']\n",
    "        \n",
    "        daylight = 2 if copenhagen_tz.dst(ts_datetime, is_dst=None) else 1\n",
    "        sunrise  = datetime.strptime(r['sunrise'], '%I:%M:%S %p').time()\n",
    "        sunset   = datetime.strptime(r['sunset'], '%I:%M:%S %p').time()\n",
    "        \n",
    "        sunrise_hour = sunrise.hour + sunrise.minute / 60.0 + daylight\n",
    "        sunset_hour  = sunset.hour + sunset.minute / 60.0 + daylight\n",
    "        \n",
    "        daylight_factors.append((sunrise_hour, sunset_hour))\n",
    "        \n",
    "    return daylight_factors\n",
    "\n",
    "def calculate_penman_monteith(temp, rh, ws, rn, pres, timeseries):\n",
    "    \"\"\"\n",
    "    Calculates Penman-Monteith potential evapotranspiration.\n",
    "    \"\"\"\n",
    "    # Copy net radiation (rn) for calculating G flux\n",
    "    G = copy.deepcopy(rn)\n",
    "    daylight_factors = get_daylight_factors(timeseries)\n",
    "    \n",
    "    # Adjust G flux based on daylight hours\n",
    "    for i, (sunrise, sunset) in enumerate(daylight_factors):\n",
    "        hour = pd.to_datetime(timeseries[i]).hour + pd.to_datetime(timeseries[i]).minute / 60\n",
    "        if sunrise <= hour < sunset:\n",
    "            G[i, :, :] = 0.1 * rn[i, :, :]\n",
    "        else:\n",
    "            G[i, :, :] = 0.5 * rn[i, :, :]\n",
    "\n",
    "    # Constants and derived values\n",
    "    CP = 1.013e-3                                       # Specific heat of air [MJ kg-1 °C-1]\n",
    "    λ  = 2.501 - 0.002361 * temp                        # Latent heat of vaporization [MJ kg-1]\n",
    "    γ  = CP * pres / (0.622 * λ)                        # Psychrometric constant [kPa °C-1]\n",
    "    es = 0.6108 * np.exp(17.27 * temp / (temp + 237.3)) # Saturation vapor pressure [kPa]\n",
    "    ea = rh / 100 * es                                  # Actual vapor pressure [kPa]\n",
    "    s  = (4098 * es) / (temp + 237.3) ** 2              # Slope of saturation vapor pressure curve [kPa °C-1]\n",
    "    pet_factor = 37\n",
    "    \n",
    "    # Wind speed adjustment to 2 m height\n",
    "    u2 = (4.87 / np.log(67.8 * 10 - 5.42)) * ws # Adjusted wind speed at 2 m height\n",
    "\n",
    "    # Penman-Monteith equation (hourly)\n",
    "    pm_pet = (0.408 * s * (rn - G) + γ * pet_factor / (temp + 273) * u2 * (es - ea)) / (s + γ * (1 + 0.34 * u2))\n",
    "\n",
    "    # Clip negative values\n",
    "    return pm_pet.clip(min=0)\n",
    "\n",
    "def calculate_makkink(temp, rs, pres):\n",
    "    \"\"\"\n",
    "    Calculates Makkink potential evapotranspiration.\n",
    "    \"\"\"\n",
    "    # Constants and derived values\n",
    "    CP = 1.013e-3                                       # Specific heat of air [MJ kg-1 °C-1]\n",
    "    λ  = 2.501 - 0.002361 * temp                        # Latent heat of vaporization [MJ kg-1]\n",
    "    γ  = (CP * pres) / (0.622 * λ)                      # Psychrometric constant [kPa °C-1]\n",
    "    es = 0.6108 * np.exp(17.27 * temp / (temp + 237.3)) # Saturation vapor pressure [kPa]\n",
    "    s  = (4098 * es) / (temp + 237.3) ** 2              # Slope of saturation vapor pressure curve [kPa °C-1]\n",
    "    \n",
    "    # Makkink equation\n",
    "    makkink_pet = (0.65 * s * rs) / (λ * (s + γ))\n",
    "    \n",
    "    # Clip negative values \n",
    "    return makkink_pet.clip(min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38f8ff-64d4-472f-9cbf-68c157e5f015",
   "metadata": {},
   "source": [
    "For more information about the equations, parameters and units, please check out FAO [documentation](https://www.fao.org/4/x0490e/x0490e05.htm#TopOfPage). These are setup for calculating PET on hourly basis, which is also the timestep interval for the HARMONIE forecast. Next, we simply call the functions to create a makkink and penman-monteith data variable. These will represent the hourly potential evapotranspiration in milimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cf056-9295-482c-ab71-a6997217394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['makkink'] = calculate_makkink(\n",
    "    temp = ds['temp'], \n",
    "    rs   = ds['grad'], \n",
    "    pres = ds['pres'],\n",
    ")\n",
    "\n",
    "ds['penman_monteith'] = calculate_penman_monteith(\n",
    "    temp       = ds['temp'], \n",
    "    rh         = ds['rh'], \n",
    "    ws         = ds['ws'], \n",
    "    rn         = ds['rn'], \n",
    "    pres       = ds['pres'], \n",
    "    timeseries = ds['time'].values, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c65aa-e1de-430a-ba0b-cf8c093a46b0",
   "metadata": {},
   "source": [
    "## 6. Saving as dfs2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebe9fb-2d78-4b6c-8f41-0671d361b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting units to match mikeio available units\n",
    "ds['grad']   = ds['grad']  * 1000         # Megajoul to Kilojoul\n",
    "ds['rn']     = ds['rn']    * 1000         # Megajoul to Kilojoul\n",
    "ds['nswrf']  = ds['nswrf'] * 1.00E-03     # Joul to Kilojoul\n",
    "ds['nlwrf']  = ds['nlwrf'] * 1.00E-03     # Joul to Kilojoul\n",
    "ds['precip'] = ds['precip'].clip(min = 0) # Clipping negative precip to 0\n",
    "\n",
    "# Define mikeio geometry and timeseries\n",
    "geometry  = mikeio.Grid2D(x=ds.x.data, y=ds.y.data[::-1], projection=\"NON-UTM\")\n",
    "timesteps = pd.DatetimeIndex(ds.time)\n",
    "timestamp = datetime.strptime(modelrun, '%Y-%m-%dT%H%M%SZ')\n",
    "timestamp = timestamp.strftime(\"%Y-%m-%dT%H\") \n",
    "\n",
    "# Export climate variables as dfs2 files\n",
    "for par in param:\n",
    "    # Invert y-axis\n",
    "    for i in range(0,len(ds.time)):\n",
    "        ds[par].data[i,:,:] = np.flipud(ds[par].data[i,:,:])\n",
    "    \n",
    "    # Create mikeio dataarray   \n",
    "    ds_mikeio = mikeio.DataArray(\n",
    "        data = ds[par].data,  # Data\n",
    "        time = timesteps,     # Time steps\n",
    "        geometry = geometry,  # Grid geometry\n",
    "        item = ItemInfo(\n",
    "            param[par][2],    # EUMLable\n",
    "            param[par][0],    # EUMType\n",
    "            param[par][1],    # EUMUnit\n",
    "            param[par][3],    # EUMTimeseries Type\n",
    "        )\n",
    "    )   \n",
    "    ds_mikeio.to_dfs(f\"{output_folder}/{par}_{timestamp}.dfs2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c75218-71d3-4c72-b638-bf3d49110f14",
   "metadata": {},
   "source": [
    "This final code block prepares and exports climate data to .dfs2 format using mikeio. Each variable is processed individually, converted to appropriate units, and clipped if necessary, then saved in a format compatible with MIKE models. The variables `grad` and `rn` are converted from Megajoules to Kilojoules, `nswrf` and `nlwrf` are converted from Joules to Kilojoules and `precip` is clipped to remove negative values.  \n",
    "The grid `geometry` uses the mikeio.Grid2D class to create a grid based on x and y coordinates, with the y-axis inverted to match MIKE’s format.  \n",
    "The `timesteps` is a DatetimeIndex that matches the times in `ds`.  \n",
    "For each parameter in param, the data is inverted along the y-axis (`np.flipud`) to align with the .dfs2 format requirements. A mikeio.DataArray is created to package the data along with time steps and grid geometry. The ItemInfo class is used to specify details like the unit label, type, and time series type for each variable, which is necessary for MIKE to interpret the file. Finally, the DataArray is saved to .dfs2 format with a filename based on the parameter name and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e8ed5-a350-427f-8df8-ec2a940e0612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
